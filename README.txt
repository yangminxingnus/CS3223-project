

1. Join Algorithms
Each Join algorithm contains three methods: open, next and close. Open method initializes variables, and finds out the index of the attribute to be joined, and materialize the right table into a file for future reading in. Next method selects the satisfying tuple using different join algorithms. In our team, the different join algorithms are Nested Join, Block Nested Join and Hash Join. Every time when next is called, a page of result will be returned.  
1.1 Block Nested Join
In Block Nested Join, the buffer is divided into 3 parts. 1 buffer page is used for loading one page of the right table, and another 1 page is used for producing output, while the rest are used to load the left table as a block of pages, which means each block is of size of the total number of buffers ¨C 2. Once a block is loaded, the right table will be loaded page by page. Then for each page in the block, each tuple in it will be evaluated and join with each tuple in right table¡¯s page. After the whole block is processed and joined with the right table, the next block will be loaded and processed. 
The cost of block nested join will be the outer total pages + celling of outer total pages/number of buffers ¨C 2 * inner total pages. 
1.2 Hash Join
In Hash Join, since I/O cost of hash join is independent of buffer size, we didn¡¯t consider buffer sizes in the implementation of hash join, but used three hash tables for implementation. There are two phases in hash join algorithm: partition and probe phases. 
In partitioning phase, we load left table page by page, and for each tuple in the pages, we calculate its hash key value, and put it in a hash table called leftHash. Similarly for the right table, we hash every tuple into a hash table called rightHash using the same hash function. The hash function is by using java hashcode() function. We get the hash code the data of the join attribute, and divide it using a prime number. We also maintain an array list of the hash key values of the left table, which is used in probing phase.
In probing/joining phase, for each hash key value of the left table, we get its corresponding list of pages in the left table, and hash each tuple into a new hash table called probeHash using another hash function. The new hash function uses a different prime value. Subsequently, we retrieve the hash key¡¯s corresponding list of pages from the right table (if exist).  For each tuple in the current list of right pages, we get its hash value using the hash function for probeHash. Subsequently, each left tuple in probeHash that has the hash value will be compared with the current right tuple, and correct join tuples will be populated. 
The cost of hash join will be 3*(the number of left pages + the number of right pages).
2. DISTINCT Implementation
Following the instruction provided on the module website, our team has setup the new parser and the isDistinct variable in SQLQuery. To make distinct function work, we have modified the class project. After the result tables are constructed by other operators, we decide whether to eliminate duplicate by setting up an extra variable in project. Query optimizer will check if duplicate elimination is required, and passes in a boolean variable to project operator. Project operator uses its pre-existing implementation if duplicate elimination is not needed, otherwise it will use a hash table and checks if the tuple has already been populated. 
To do that, the idea of hash join is again used here. Here we only check on the attributes to be projected and ignore other attributes. Tuples will be hashed into different groups by the hash value of its first attribute. Later when a tuple with an existing first attribute value came, project operator will check if the tuple contains the same value as any of the tuples in the group. If the tuple is a new tuple, it will be added to the output and hash table, otherwise it will be discarded.
3. Query Optimizer
The logic of dynamic query optimizer is the same from the lecture notes.
Upon calling to be requested for an optimal plan, this dynamic query optimizer will do an bottom-up trace to get the optimal plan.
The first step is to build the basic layer which is composed by all size-1 relations. By building it mean to attach scan operators and select operators(if exist) to each single table. 
Then the result is stored in a variable called subsetSpace. This variable will store all the subsets of the query tables. We start from dealing with level 0 (which contains all size-1 tables.), then go to the up layers gruadually. 
Starting with size-2 combinations of the tables, the optimizer will firstly compute all the possible subsets of current level size(level 0 is size1, level1 is size2, etc), then put them in the subsetSpace. The method it used to compute all the combinations is to use binary bits.
Starting from 0, it increases this count by 1 for each round, then convert it into binary format, and now the ¡°1¡± bits are representing the presentation of the table in the corresponding position. And when stop at tableNum ^ 2, it is guaranteed that all combinations are checked.
For example, if we have a table of size 3, we will count 001, 010, 011, 100, 101, 110, 111.
And if we want all the combination of 2 tables in this subsetSpace, we will take the ones with two ¡°1¡±s.
Subsequently, it will compute all the subset plan cost by calling the method accessPlan . And this method will make use of the previous level information to compute the cost for current level. Since the number of table is increasing only by one from level to level, so we will have enough information to compute all the cost in this bottom-up approach. 
And in this process, a method called getJoinCondition will be called, this method will iterate through the joinList in sqlquery, and attach join operators for the subset plan. Also, since we are in a bottom-up approach, it can be guaranteed that all join operations will be attached. And it will return the root operator of this join.
The dynamic query optimizer will repeat above process until it reaches the level where the size of subset relation in the previous is N - 1 (N is the total number of tables). After it computes the last level costs, it will return the smallest one. (by returning the root operator)
